apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: tfserving-resnet
  labels:
    inference_framework: kserve
    ML_framework: tensorflow
    model_server: tfserving
spec:
  predictor:
    containers:
      - name: tfserving_resnet_container
        image: mehransi/main:tfserving_resnet
        ports:
          - containerPort: 8501
          - containerPort: 8500
        args:
          - --model_config_file=/etc/tfserving/models.config
          - --model_config_file_poll_wait_seconds=10  # Use HandleReloadConfigRequest RPC request instead
          - --enable_batching=true
          - --batching_parameters_file=/etc/tfserving/batch.config
        resources:
          requests:
            memory: "2Gi"
            cpu: "2"
          limits:
            memory: "2Gi"
            cpu: "2"
        volumeMounts:
          - name: tfserving-resnet-vol
            mountPath: /etc/tfserving
    volumes:
      - name: tfserving-resnet-vol
        configMap:
          name: tfserving-resnet-cm
